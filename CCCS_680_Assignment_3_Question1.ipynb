{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelmalki/MichelMalki/blob/main/CCCS_680_Assignment_3_Question1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Setup and CSV Input\n",
        "\n",
        "The following notebook helps to setup pyspark on Colab as shows you how to read and extract contents from a CSV file.\n",
        "\n",
        "First we download and install Java, Spark and findspark"
      ],
      "metadata": {
        "id": "vizXrrZLSAk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"CCCS 680 Assignment 3 Question 1 Template\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "5EzVOJRKSODe",
        "outputId": "fec80e98-00c3-4f57-f0dd-07b3ace27ff8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 5s (50.9 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x794ef3360190>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6d4cf3e7a6b3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>CCCS 680 Assignment 3 Question 1 Template</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we map our google drive. Note that you will be prompted for your google account credentials and be provided with an authorization code to insert. The contents of your google drive storage will be mounted at `/content/drive`."
      ],
      "metadata": {
        "id": "mjMfegM1Su-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABiQiXcXSzs3",
        "outputId": "6c79cbe6-2cee-43da-fd2a-d64b1453bb2f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we start our Spark Python application. We will try reading the Sales Data file. Note that Spark can read .gz files directly and can do an amazing job of inferring table schema from CSV files."
      ],
      "metadata": {
        "id": "9LOlvBZYTGDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "purchases_df = spark.read.csv('/content/drive/My Drive/CCCS-680 Assignment 3/purchases.txt.gz', inferSchema=True, header=False, sep='\\t')"
      ],
      "metadata": {
        "id": "i1EW3_ZqTP6r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if it worked"
      ],
      "metadata": {
        "id": "UQ7US3oQVX_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "purchases_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxY5xk0aVa0e",
        "outputId": "f14bd428-4559-4311-dd63-b173531db09b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+--------------+--------------------+------+----------+\n",
            "|       _c0|                _c1|           _c2|                 _c3|   _c4|       _c5|\n",
            "+----------+-------------------+--------------+--------------------+------+----------+\n",
            "|2012-01-01|2024-07-11 09:00:00|      San Jose|      Men's Clothing|214.05|      Amex|\n",
            "|2012-01-01|2024-07-11 09:00:00|    Fort Worth|    Women's Clothing|153.57|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|     San Diego|               Music| 66.08|      Cash|\n",
            "|2012-01-01|2024-07-11 09:00:00|    Pittsburgh|        Pet Supplies|493.51|  Discover|\n",
            "|2012-01-01|2024-07-11 09:00:00|         Omaha| Children's Clothing|235.63|MasterCard|\n",
            "|2012-01-01|2024-07-11 09:00:00|      Stockton|      Men's Clothing|247.18|MasterCard|\n",
            "|2012-01-01|2024-07-11 09:00:00|        Austin|             Cameras| 379.6|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|      New York|Consumer Electronics| 296.8|      Cash|\n",
            "|2012-01-01|2024-07-11 09:00:00|Corpus Christi|                Toys| 25.38|  Discover|\n",
            "|2012-01-01|2024-07-11 09:00:00|    Fort Worth|                Toys|213.88|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|     Las Vegas|         Video Games| 53.26|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|        Newark|         Video Games| 39.75|      Cash|\n",
            "|2012-01-01|2024-07-11 09:00:00|        Austin|             Cameras|469.63|MasterCard|\n",
            "|2012-01-01|2024-07-11 09:00:00|    Greensboro|                DVDs|290.82|MasterCard|\n",
            "|2012-01-01|2024-07-11 09:00:00| San Francisco|               Music|260.65|  Discover|\n",
            "|2012-01-01|2024-07-11 09:00:00|       Lincoln|              Garden| 136.9|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|       Buffalo|    Women's Clothing|483.82|      Visa|\n",
            "|2012-01-01|2024-07-11 09:00:00|      San Jose|    Women's Clothing|215.82|      Cash|\n",
            "|2012-01-01|2024-07-11 09:00:00|        Boston|             Cameras|418.94|      Amex|\n",
            "|2012-01-01|2024-07-11 09:00:00|       Houston|                Baby|309.16|      Visa|\n",
            "+----------+-------------------+--------------+--------------------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.A\n",
        "\n",
        "What was the total amount sold"
      ],
      "metadata": {
        "id": "DrU2GnV1Wbfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "purchases_df.select(F.sum(purchases_df._c4)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHoi_-_XV7Ua",
        "outputId": "f68dc8af-f7b0-413b-927b-f9473fa8a5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            sum(_c4)|\n",
            "+--------------------+\n",
            "|1.0344579532600112E9|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.B\n",
        "What is the total number of transactions per store location?\n"
      ],
      "metadata": {
        "id": "74g8_yUhWvgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = purchases_df.groupBy(purchases_df._c2).count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXWH3vFaWuzn",
        "outputId": "0cbcee83-f381-4e50-db4d-254e017f874f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|            _c2|count|\n",
            "+---------------+-----+\n",
            "|North Las Vegas|40013|\n",
            "|        Phoenix|40333|\n",
            "|          Omaha|40209|\n",
            "|      Anchorage|39806|\n",
            "|        Anaheim|40086|\n",
            "|     Greensboro|40232|\n",
            "|         Dallas|40368|\n",
            "|        Oakland|39728|\n",
            "|         Laredo|40342|\n",
            "|     Scottsdale|40173|\n",
            "|    San Antonio|40197|\n",
            "|    Bakersfield|40326|\n",
            "|        Raleigh|40261|\n",
            "|    Chula Vista|40080|\n",
            "|   Philadelphia|40748|\n",
            "|     Louisville|40099|\n",
            "|    Los Angeles|40254|\n",
            "|       Chandler|39826|\n",
            "|     Sacramento|40561|\n",
            "|   Indianapolis|40321|\n",
            "+---------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = purchases_df.sum('purchases_df._c2', 'purchases_df._c3').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "7gOwbyL4VfO9",
        "outputId": "c35b39b6-b3e3-4820-de92-c2f9f6b5a54b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'sum'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-952cfbc03a96>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpurchases_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'purchases_df._c2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'purchases_df._c3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3125\u001b[0m         \"\"\"\n\u001b[1;32m   3126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3127\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3128\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sum'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Group by product type (_c3) and store location (_c2) and sum the amount sold (_c4)\n",
        "result = purchases_df.groupBy(\"_c3\", \"_c2\").agg(F.sum(\"_c4\").alias(\"total_sold\"))\n",
        "\n",
        "# Show the result\n",
        "result.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZH8NNs_XA-5",
        "outputId": "c620bd58-4018-44e2-c541-0a0a7c258b5f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------+-----------------+\n",
            "|                _c3|         _c2|       total_sold|\n",
            "+-------------------+------------+-----------------+\n",
            "|  Health and Beauty|   Charlotte|547280.5699999997|\n",
            "|             Crafts|      Durham|553803.5200000005|\n",
            "|   Women's Clothing|        Mesa|572868.2699999996|\n",
            "|            Cameras|       Plano|548115.3300000005|\n",
            "|Children's Clothing| Los Angeles|574671.8499999995|\n",
            "|       Pet Supplies|  Fort Wayne|560785.3400000004|\n",
            "|                CDs|Indianapolis|572676.8100000003|\n",
            "|     Men's Clothing| Baton Rouge|563814.1699999993|\n",
            "|   Women's Clothing|      Aurora|551525.0699999997|\n",
            "|  Health and Beauty|      Aurora|552101.0300000011|\n",
            "|     Sporting Goods|     Fremont| 544994.409999999|\n",
            "|             Crafts| Albuquerque|543748.6199999994|\n",
            "|            Cameras| Bakersfield|547924.4799999984|\n",
            "|            Cameras|     El Paso|        581664.08|\n",
            "|          Computers|Indianapolis|560813.3699999992|\n",
            "|                CDs|     Lubbock|521722.7599999999|\n",
            "|        Video Games|      Durham|563656.4600000015|\n",
            "|             Garden|      Austin|582794.6499999998|\n",
            "|       Pet Supplies|Jacksonville|539193.8699999994|\n",
            "|  Health and Beauty|     Garland|552989.7900000009|\n",
            "+-------------------+------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = purchases_df.groupBy(purchases_df._c2).max().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2oV9wOqUkeE",
        "outputId": "f7057756-5554-4147-b8f1-18bb31d3e0ea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+\n",
            "|            _c2|max(_c4)|\n",
            "+---------------+--------+\n",
            "|North Las Vegas|  499.98|\n",
            "|        Phoenix|  499.99|\n",
            "|          Omaha|  499.99|\n",
            "|      Anchorage|  499.99|\n",
            "|        Anaheim|  499.98|\n",
            "|     Greensboro|  499.99|\n",
            "|         Dallas|  499.99|\n",
            "|        Oakland|  499.99|\n",
            "|         Laredo|  499.96|\n",
            "|     Scottsdale|  499.98|\n",
            "|    San Antonio|  499.98|\n",
            "|    Bakersfield|  499.97|\n",
            "|        Raleigh|  499.99|\n",
            "|    Chula Vista|  499.99|\n",
            "|   Philadelphia|  499.98|\n",
            "|     Louisville|  499.98|\n",
            "|    Los Angeles|  499.99|\n",
            "|       Chandler|  499.98|\n",
            "|     Sacramento|  499.96|\n",
            "|   Indianapolis|  499.98|\n",
            "+---------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}